
{
  "experience": [
    {
      "id": 1,
      "date": "2022 - Present",
      "title": "Data Engineer",
      "company": "Paypal",
      "description": "Led the end-to-end migration of data pipelines from SAS to Teradata and Teradata to BigQuery, enabling cloud-optimized analytics at scale. Optimized BigQuery scripts, improving runtime by 40% and cutting query costs by 25%. Built a data latency monitoring tool to ensure data freshness and transparency for stakeholders. Contributed across Sanctions, CIP, and Seller Risk domains, aligning technical solutions with business goals. Recognized with a team shout-out at GFCCP for leading critical pipeline module migration and developing a reusable batch monitoring framework that accelerated delivery and boosted operational visibility.",
      "skills": ["Python", "Apache Spark", "Google Cloud Platform", "Apache Airflow", "BigQuery", "Docker"],
      "comments": [
        "Performed end-to-end migration of data pipelines from SAS to Teradata and Teradata to BigQuery, enabling scalable and cloudoptimized analytics.",
        "Optimized BigQuery scripts for a critical project, improving runtime and CPU efficiency by 40% and reducing query costs by 25%",
        "Developed a data latency report and monitoring tool to track data freshness and proactively inform customers about the availability of latest data.",
        "Worked across Sanctions, CIP, and Seller Risk domains, driving technical improvements while aligning data solutions with stakeholder business goals."
      ]
    },
    {
      "id": 2,
      "date": "2021 - 2022",
      "title": "Development Engineer 2",
      "company": "Comcast India Engineering Center",
      "description": "Led the end-to-end migration of data from flat files into a centralized Data Integration Layer, implementing complex ETL logic tailored to business requirements. Developed secure Pentaho jobs to ingest encrypted files into Oracle tables and optimized SQL performance through strategic indexing. Designed and deployed a reusable Python-based File Validation Framework that significantly improved data quality and reduced production support by 20%, while boosting processing efficiency by 35%.",
      "skills": ["Python", "AWS", "Git", "Oracle SQL", "Pentaho", "Rundeck", "Shell Scripting"],
      "comments": [
        "Built real-time streaming pipelines processing 1M+ events/hour",
        "Implemented data quality framework reducing data errors by 90%",
        "Optimized ETL processes saving $200K annually in compute costs",
        "Created automated monitoring system preventing 95% of pipeline failures"
      ]
    },
    {
      "id": 3,
      "date": "2018 - 2021",
      "title": "System Engineer",
      "company": "Tata Consultancy Services",
      "description": "Designed and optimized complex SQL logic in BTEQ to streamline ETL processes, significantly reducing code complexity. Managed high-volume data workflows across 150+ tables and conducted detailed Root Cause Analysis (RCA) to resolve runtime failures efficiently. Engineered robust data pipelines to extract and migrate data from Oracle, Teradata, and Oracle GoldenGate into Flat Files and Teradata targets. Built a custom Python framework to decrypt and load data using Teradata TPT, and developed CARNOT — a real-time alerting framework to ensure end-to-end data quality with zero loss. Also implemented BigQuery pipelines to process and parse JSON payloads into analytics-ready tables.",
      "skills": ["Python", "SQL", "UC4", "MySQL", "Control-M", "Pandas", "NumPy", "Jupyter", "Teradata", "BigQuery"],
      "comments": [
        "Automated 15+ manual data processes saving 40 hours/week",
        "Built data validation framework detecting 99% of data anomalies",
        "Created interactive dashboards for business stakeholders",
        "Optimized database queries improving performance by 70%"
      ]
    }
  ],
  "skills": [
    {
      "category": "Programming Languages",
      "icon": "fas fa-code",
      "skills": ["Python", "SQL", "Shell Scripting" , "Pyspark"]
    },
    {
      "category": "Data Processing & Analytics",
      "icon": "fas fa-database",
      "skills": ["Apache Spark", "Apache Kafka", "Apache Beam", "Pandas"]
    },
    {
      "category": "Cloud Platforms",
      "icon": "fas fa-cloud",
      "skills": ["Google Cloud Platform", "Amazon Web Services", "Snowflake"]
    },
    {
      "category": "Orchestration & Workflow",
      "icon": "fas fa-sitemap",
      "skills": ["Apache Airflow",  "AWS Step Functions", "UC4" , "Control-M" , "Rundeck" , "Crontab"]
    },
    {
      "category": "Databases & Storage",
      "icon": "fas fa-server",
      "skills": ["BigQuery", "MySQL", "MongoDB" , "Oralce SQL", "Snowflake", "Teradata"]
    },
    {
      "category": "DevOps & Infrastructure",
      "icon": "fas fa-cogs",
      "skills": ["Docker", "Kubernetes", "Git", "Linux", "Docker Compose"]
    },
    {
      "category": "Monitoring & Observability",
      "icon": "fas fa-chart-line",
      "skills": ["Grafana", "Prometheus", "Datadog",  "Tableau", "Power BI"]
    },
    {
      "category": "Data Modeling & Concepts",
      "icon": "fas fa-brain",
      "skills": ["Dimensional Modeling", "Star Schema", "Snowflake Schema", "Data Lake", "Data Mart"]
    }
  ],
  "achievements": [
  {
    "icon": "fas fa-exchange-alt",
    "title": "Cloud Migration Expert",
    "description": "Performed end-to-end migration of data pipelines from SAS to Teradata and Teradata to BigQuery, enabling scalable and cloud-optimized analytics across multiple business domains."
  },
  {
    "icon": "fas fa-bolt",
    "title": "Query Performance Optimization",
    "description": "Optimized BigQuery scripts for a critical project, improving runtime and CPU efficiency by 40% and reducing query costs by 25%."
  },
  {
    "icon": "fas fa-clock",
    "title": "Data Freshness & Monitoring",
    "description": "Developed a data latency report and monitoring tool to track data freshness and proactively notify stakeholders about the availability of the latest data."
  },
  {
    "icon": "fas fa-shield-alt",
    "title": "Data Quality Framework Innovator",
    "description": "Built a reusable Python-based File Validation Framework to validate flat files before ingestion, reducing production support by 20% and improving data quality by 35%."
  },
  {
    "icon": "fas fa-bug",
    "title": "Root Cause Analysis at Scale",
    "description": "Managed ETL workflows across 150+ tables and performed in-depth RCA for runtime failures, enhancing system reliability and operational efficiency."
  },
  {
    "icon": "fas fa-code",
    "title": "Python Automation & Alerts",
    "description": "Created CARNOT – a Python-based real-time alerting framework to notify end users about data quality issues, ensuring zero data loss post ingestion."
  },
  {
    "icon": "fas fa-key",
    "title": "Secure ETL with Encryption Handling",
    "description": "Developed a Python framework to decrypt and parse data from encrypted sources, enabling secure loading into Teradata using TPT loaders."
  },
  {
    "icon": "fas fa-project-diagram",
    "title": "Enterprise-Scale ETL Architect",
    "description": "Designed and implemented complex ETL workflows to extract data from Oracle, Teradata, and Oracle GoldenGate into flat files and warehouse targets."
  }
]

}
